## “sklearn 多项式回归”，这是指在 scikit-learn 中使用多项式特征（Polynomial Features）结合线性回归来实现多项式回归。

### 什么是多项式回归？
多项式回归是线性回归的扩展，通过引入特征的非线性变换（例如平方、立方或交叉项），使模型能够拟合非线性关系。虽然本质上仍是线性回归（因为系数是线性的），但它可以捕捉输入特征与目标变量之间的复杂关系。

#### 公式
普通线性回归模型：
$\[ y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon \]$

多项式回归（例如二次多项式）：
$\[ y = \beta_0 + \beta_1 x_1 + \beta_2 x_1^2 + \beta_3 x_2 + \beta_4 x_2^2 + \beta_5 x_1 x_2 + \epsilon \]$
- $\(x_1^2\)、\(x_2^2\)、\(x_1 x_2\)$ 是通过多项式特征生成的高阶项。
- $\(\beta_i\)$ 是模型要学习的系数。

#### 适用场景
- 数据呈现非线性趋势（例如抛物线、指数曲线等）。
- 需要比线性回归更强的表达能力，但仍想保持模型简单。

---

### 在 scikit-learn 中实现多项式回归
scikit-learn 没有直接的“多项式回归”类，而是通过以下步骤实现：
1. 使用 `PolynomialFeatures` 生成多项式特征。
2. 将生成的特征传入线性回归模型（例如 `LinearRegression` 或 `Ridge`）。
3. 训练模型并进行预测。

#### 关键类
- **`PolynomialFeatures`**：
  - 参数：
    - `degree`：多项式的最高次数（例如 2 表示二次，3 表示三次）。
    - `include_bias`：是否包含常数项（默认 True）。
    - `interaction_only`：是否只生成交互项（例如 \(x_1 x_2\)，不生成 \(x_1^2\)）。
  - 作用：将原始特征转换为包含高阶项和交互项的新特征矩阵。
- **`LinearRegression`** 或 **`Ridge`**：用于拟合转换后的特征。

---

### Python 示例
以下是一个使用 scikit-learn 实现多项式回归的完整示例：

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

# 生成模拟数据（非线性关系）
np.random.seed(42)
X = np.linspace(-3, 3, 100).reshape(-1, 1)  # 特征 X
y = 2 * X**2 - 3 * X + 1 + np.random.normal(0, 1, (100, 1))  # 二次关系 + 噪声

# 数据分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 1. 生成多项式特征（二次）
poly = PolynomialFeatures(degree=2, include_bias=False)  # 不包含常数项（LinearRegression 自带截距）
X_train_poly = poly.fit_transform(X_train)
X_test_poly = poly.transform(X_test)

# 查看转换后的特征名称
print("多项式特征名称:", poly.get_feature_names_out())

# 2. 训练线性回归模型
model = LinearRegression()
model.fit(X_train_poly, y_train)

# 3. 预测
y_pred = model.predict(X_test_poly)

# 4. 计算 R² 分数
r2 = r2_score(y_test, y_pred)
print("R² 分数:", r2)

# 5. 可视化结果
X_range = np.linspace(-3, 3, 100).reshape(-1, 1)
X_range_poly = poly.transform(X_range)
y_range_pred = model.predict(X_range_poly)

plt.scatter(X_train, y_train, color='blue', label='训练数据', alpha=0.5)
plt.scatter(X_test, y_test, color='green', label='测试数据', alpha=0.5)
plt.plot(X_range, y_range_pred, color='red', label='多项式回归拟合曲线')
plt.xlabel('X')
plt.ylabel('y')
plt.title('多项式回归 (degree=2)')
plt.legend()
plt.grid(True)
plt.show()

# 打印模型系数
print("模型系数:", model.coef_)
print("截距:", model.intercept_)
```

---

### 输出解读
#### 示例输出
```
多项式特征名称: ['x0' 'x0^2']
R² 分数: 0.925

模型系数: [[-3.05  2.12]]
截距: [1.15]
```
- **特征名称**：`x0` 是原始特征，`x0^2` 是平方项。
- **R² = 0.925**：模型解释了 92.5% 的数据方差，拟合效果很好。
- **系数**：模型近似拟合为 $\(y = 1.15 - 3.05x + 2.12x^2\)，接近真实关系 \(y = 1 - 3x + 2x^2\)。$

#### 可视化
- 散点图显示训练和测试数据。
- 红色曲线是二次多项式回归的拟合结果，很好地捕捉了数据的抛物线趋势。

---

### 结合岭回归的多项式回归
如果担心过拟合（尤其当 `degree` 较高时），可以用 `Ridge` 替换 `LinearRegression`：

```python
from sklearn.linear_model import Ridge

# 岭回归多项式模型
ridge_model = Ridge(alpha=1.0)
ridge_model.fit(X_train_poly, y_train)
y_pred_ridge = ridge_model.predict(X_test_poly)
r2_ridge = r2_score(y_test, y_pred_ridge)
print("岭回归 R² 分数:", r2_ridge)
```

#### 输出
```
岭回归 R² 分数: 0.924
```
- 岭回归通过正则化略微调整系数，可能提高模型在未知数据上的稳定性。

---

### 注意事项
1. **degree 的选择**：
   - `degree` 太低（例如 1）：模型可能欠拟合，无法捕捉非线性关系。
   - `degree` 太高（例如 5）：模型可能过拟合，R² 在训练集上很高，但在测试集上下降。
   - 通常通过交叉验证选择最佳 `degree`。
2. **特征爆炸**：
   - 如果原始特征较多（例如 10 个特征），`degree=2` 会生成大量新特征（\(C(10+2, 2) = 66\)），增加计算成本。
   - 可以用 `interaction_only=True` 只生成交互项，或结合降维方法。
3. **标准化**：
   - 多项式特征的数值范围差异可能很大（例如 \(x\) 和 \(x^2\)），建议在 `PolynomialFeatures` 后用 `StandardScaler` 标准化特征。

#### 添加标准化示例
```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_poly_scaled = scaler.fit_transform(X_train_poly)
X_test_poly_scaled = scaler.transform(X_test_poly)

model.fit(X_train_poly_scaled, y_train)
y_pred_scaled = model.predict(X_test_poly_scaled)
print("标准化后 R²:", r2_score(y_test, y_pred_scaled))
```

---

### 总结
- **多项式回归**：通过 `PolynomialFeatures` 生成高阶特征，再用线性回归拟合。
- **公式**：本质上是线性回归，特征被扩展为 \(x, x^2, x^3, \ldots\)。
- **sklearn 实现**：`PolynomialFeatures` + `LinearRegression` 或 `Ridge`。
- **评估**：用 R² 分数衡量拟合效果。

如果你有具体问题（比如调整 `degree`、处理多特征数据，或与岭回归结合的具体实现），请告诉我，我会进一步帮你解答！