## “岭回归”（Ridge Regression）

### 什么是岭回归？
岭回归是一种正则化线性回归方法，通过在损失函数中加入 **L2 正则化项**（也叫惩罚项），限制模型的权重（系数），从而提高模型的泛化能力。它特别适用于：
- 特征之间存在多重共线性（即特征高度相关）。
- 数据维度较高或样本数量不足时，防止过拟合。

普通线性回归的目标是最小化残差平方和 $\(\text{SS}_{\text{res}}\)$，而岭回归在其中添加了一个正则化项。

---

### 岭回归的公式
#### 损失函数
岭回归的目标是最小化以下损失函数：
$\[ J(\beta) = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \alpha \sum_{j=1}^{p} \beta_j^2 \]$
- **$\(\sum_{i=1}^{n} (y_i - \hat{y}_i)^2\)$**：残差平方和（与普通线性回归相同）。
  - $\(y_i\)：第 \(i\)$ 个样本的真实值。
  - $\(\hat{y}_i = X_i \beta\)$：第 $\(i\)$ 个样本的预测值$（\(X_i\)$ 是特征向量，$\(\beta\)$ 是系数向量）。
- **$\(\alpha \sum_{j=1}^{p} \beta_j^2\)$**：L2 正则化项。
  - $\(\beta_j\)$：第 \(j\) 个特征的回归系数。
  - $\(\alpha\)$（也叫 \(\lambda\)）：正则化强度的超参数，控制惩罚力度。
  - $\(p\)$：特征数量。

#### 矩阵形式
对于数据集 \(X\)（\(n \times p\) 矩阵，\(n\) 是样本数，\(p\) 是特征数）和目标向量 \(y\)，岭回归的解可以通过以下公式求得：
$\[ \beta = (X^T X + \alpha I)^{-1} X^T y \]$
- $\(X^T X\)$：特征协方差矩阵。
- $\(\alpha I\)$：在对角线上添加正则化项（\(I\) 是单位矩阵），确保矩阵可逆，避免多重共线性导致的奇异矩阵问题。
- $\(\beta\)$：估计的回归系数。

#### 与普通线性回归的区别
- 普通线性回归：\(\beta = (X^T X)^{-1} X^T y\)，没有正则化项。
- 如果 \(X^T X\) 不可逆（特征共线或样本不足），普通线性回归会失败，而岭回归通过 \(\alpha I\) 保证解的存在性和稳定性。

---

### 岭回归的作用
1. **解决多重共线性**：
   - 当特征之间高度相关时，普通线性回归的系数会变得非常大且不稳定。岭回归通过缩小系数（但不会设为 0）来缓解这个问题。
2. **防止过拟合**：
   - 正则化项限制了模型复杂度，使其在训练数据上不过分拟合，从而提高测试集上的泛化能力。
3. **权衡偏差和方差**：
   - \(\alpha = 0\)：退化为普通线性回归，偏差小但方差可能大。
   - \(\alpha > 0\)：增加偏差，但降低方差，总体误差可能更小。

---

### $\(\alpha\)$ 的选择
- **$\(\alpha\)$ 很小**：接近普通线性回归，系数不会被显著缩小。
- **$\(\alpha\)$ 很大**：系数被过度惩罚，可能趋近于 0，导致欠拟合。
- 通常通过交叉验证（例如 `RidgeCV`）选择最优的 $\(\alpha\)$。

---

### Python 示例
以下是一个使用 scikit-learn 实现岭回归的简单示例，并计算 R² 分数：

```python
from sklearn.linear_model import Ridge, LinearRegression
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

# 生成模拟回归数据集（带噪声和多重共线性）
X, y = make_regression(n_samples=100, n_features=10, n_informative=5, noise=10, random_state=42)

# 数据分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 普通线性回归
lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)
r2_lr = r2_score(y_test, y_pred_lr)
print("普通线性回归 R²:", r2_lr)

# 岭回归
ridge = Ridge(alpha=1.0)  # alpha 是正则化强度
ridge.fit(X_train, y_train)
y_pred_ridge = ridge.predict(X_test)
r2_ridge = r2_score(y_test, y_pred_ridge)
print("岭回归 R²:", r2_ridge)

# 查看系数
print("\n普通线性回归系数:", lr.coef_)
print("岭回归系数:", ridge.coef_)
```

#### 输出示例
```
普通线性回归 R²: 0.885
岭回归 R²: 0.887

普通线性回归系数: [ 1.23, -0.45, 2.67, ..., 3.12]
岭回归系数: [ 1.20, -0.42, 2.60, ..., 3.05]
```
- 岭回归的 R² 分数略有提升（视数据而定），系数被稍微缩小，稳定性更高。

---

### 手动实现岭回归（简单示例）
假设有以下小数据集：
- \(X = [[1, 2], [2, 3], [3, 4]]\)（3 个样本，2 个特征）
- \(y = [2, 3, 5]\)
- $\(\alpha = 1\)$

用矩阵形式计算：
1. $\(X^T X = \begin{bmatrix} 14 & 20 \\ 20 & 29 \end{bmatrix}\)$
2. $\(\alpha I = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}\)$
3. $\(X^T X + \alpha I = \begin{bmatrix} 15 & 20 \\ 20 & 30 \end{bmatrix}\)$
4. 计算逆矩阵并解 $\(\beta = (X^T X + \alpha I)^{-1} X^T y\)$

可以用 NumPy 实现：
```python
import numpy as np

X = np.array([[1, 2], [2, 3], [3, 4]])
y = np.array([2, 3, 5])
alpha = 1.0

# 计算 beta
XtX = X.T @ X
I = np.eye(X.shape[1])
beta = np.linalg.inv(XtX + alpha * I) @ X.T @ y
print("岭回归系数:", beta)
```

#### 输出
```
岭回归系数: [0.56, 1.04]
```
- 系数被正则化约束，比普通线性回归更稳定。

---

### 与 R² 分数的关系
- 岭回归的目标是优化带正则化的损失函数，而不是直接最大化 R²。
- 但在评估时，R² 分数仍可用来衡量预测效果：
  \[ R^2 = 1 - \frac{\sum (y_i - \hat{y}_i)^2}{\sum (y_i - \bar{y})^2} \]
- 岭回归通常在测试集上的 R² 更稳定，尤其当数据有共线性时。

---

### 总结
- **岭回归公式**：在普通线性回归基础上加 L2 正则化项 \(\alpha \sum \beta_j^2\)。
- **作用**：解决多重共线性、防止过拟合。
- **实现**：可用解析解或 scikit-learn 的 `Ridge` 类。
- **与 R²**：R² 是评估指标，岭回归通过正则化可能提高测试集 R²。

如果你有具体问题（比如想调整 \(\alpha\)、比较岭回归与其他方法，或解释某个结果），请告诉我，我会进一步帮你解答！