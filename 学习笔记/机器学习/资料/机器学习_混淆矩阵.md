## 混淆矩阵

### 什么是混淆矩阵？
混淆矩阵（Confusion Matrix）是一个用于评估分类模型性能的工具，尤其在监督学习中非常常见。它以表格形式展示模型预测结果与真实标签之间的关系，帮助你直观地看到模型在每个类别上的表现。

#### 二分类混淆矩阵
对于二分类问题（例如类别 0 和 1），混淆矩阵是一个 2x2 的表格：

|                | 预测正类 (1) | 预测负类 (0) |
|----------------|--------------|--------------|
| **真实正类 (1)** | TP (真阳性)  | FN (假阴性)  |
| **真实负类 (0)** | FP (假阳性)  | TN (真阴性)  |

- **TP (True Positive)**：真实为正类，预测也为正类。
- **TN (True Negative)**：真实为负类，预测也为负类。
- **FP (False Positive)**：真实为负类，但预测为正类（误报）。
- **FN (False Negative)**：真实为正类，但预测为负类（漏报）。

#### 多分类混淆矩阵
对于多分类问题，混淆矩阵是一个 NxN 的表格（N 是类别数），对角线表示每个类别正确预测的样本数，非对角线表示错误预测的样本数。

### 混淆矩阵的作用
1. **评估模型性能**：
   - 计算准确率：`(TP + TN) / (TP + TN + FP + FN)`。
   - 计算精确率（Precision）：`TP / (TP + FP)`。
   - 计算召回率（Recall）：`TP / (TP + FN)`。
   - 计算 F1 分数：`2 * (Precision * Recall) / (Precision + Recall)`。
2. **发现模型偏见**：例如，模型是否倾向于预测多数类。
3. **分析不平衡数据**：结合你之前的 SMOTE 和 RandomUnderSampler，混淆矩阵可以帮助你评估处理不平衡数据后的效果。

### 在你的上下文中使用混淆矩阵
从你之前的代码来看，你在处理不平衡数据集（用 SMOTE 上采样和 RandomUnderSampler 下采样），然后可能用 KNN 或其他分类器训练模型。假设你已经训练了一个模型，现在想用混淆矩阵来评估它，我们可以这样做：

#### 示例代码
以下是一个完整的示例，基于你之前的代码背景，展示如何生成和解读混淆矩阵：

```python
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, classification_report
import numpy as np
from collections import Counter

# 假设 dataset 是你的数据集
X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2, random_state=3)
print("原始训练集类别分布:", Counter(y_train))


# 训练模型（以 KNN 为例）
knn = KNeighborsClassifier(n_neighbors=min(5, len(X_train)))  # 动态调整 n_neighbors
knn.fit(X_train, y_train)  # 使用 SMOTE 后的数据训练

# 预测测试集
y_pred = knn.predict(X_test)

# 生成混淆矩阵
cm = confusion_matrix(y_test, y_pred)
print("混淆矩阵:\n", cm)

# 打印分类报告（包含精确率、召回率、F1 分数等）
print("\n分类报告:\n", classification_report(y_test, y_pred))
```

#### 输出解读
假设你的 `y_test` 和 `y_pred` 是二分类问题，混淆矩阵可能是这样的：
```
混淆矩阵:
[[50  5]  # 真实负类 (0)
 [10 35]] # 真实正类 (1)
```
- **解读**：
  - `TN = 50`：50 个负类样本被正确预测为负类。
  - `FP = 5`：5 个负类样本被错误预测为正类。
  - `FN = 10`：10 个正类样本被错误预测为负类。
  - `TP = 35`：35 个正类样本被正确预测为正类。

`classification_report` 会进一步提供：
```
              precision    recall  f1-score   support
       0       0.83      0.91      0.87        55
       1       0.88      0.78      0.82        45
accuracy                           0.85       100
```
- **精确率 (Precision)**：预测为正类的样本中，真正为正类的比例。
- **召回率 (Recall)**：真实正类中被正确预测的比例。
- **F1 分数**：精确率和召回率的调和平均数。
- **准确率 (Accuracy)**：整体正确预测的比例。

### 可视化混淆矩阵
为了更直观地展示混淆矩阵，可以用 `seaborn` 绘制热力图：
```python
import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['负类', '正类'], yticklabels=['负类', '正类'])
plt.xlabel('预测标签')
plt.ylabel('真实标签')
plt.title('混淆矩阵')
plt.show()
```

### 1. F1 分数
#### 定义
F1 分数是精确率（Precision）和召回率（Recall）的调和平均数，用于评估分类模型在特定阈值下的性能：
$$ F1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} $$
- **精确率 (Precision)** = $$\frac{TP}{TP + FP}$$，预测为正类的样本中正确预测的比例。
- **召回率 (Recall)** = $$\frac{TP}{TP + FN}$$，真实正类中被正确预测的比例。
- **TP**：真阳性，**FP**：假阳性，**FN**：假阴性。

#### 作用
- F1 分数平衡了精确率和召回率，特别适合不平衡数据集或需要关注正类（例如少数类）的场景。
- 依赖于混淆矩阵，因此与分类阈值（通常默认 0.5）相关。

#### 示例
假设一个二分类模型的混淆矩阵为：
```
[[90 10]  # 负类 (0)
 [20 80]] # 正类 (1)
```
- 正类 (1)：
  - $$ TP = 80 $, $ FP = 10 $, $ FN = 20 $$
  - $$ \text{Precision} = \frac{80}{80 + 10} = 0.89 $$
  - $$ \text{Recall} = \frac{80}{80 + 20} = 0.80 $$
  - $$ F1 = 2 \times \frac{0.89 \times 0.80}{0.89 + 0.80} = 2 \times \frac{0.712}{1.69} \approx 0.84 $$

---

### 2. ROC 曲线
#### 定义
ROC 曲线（接收者操作特征曲线）展示模型在不同分类阈值下的表现：
- **横轴**：假阳性率 (FPR) = $\frac{FP}{FP + TN}$。
- **纵轴**：真阳性率 (TPR, 即召回率) = $\frac{TP}{TP + FN}$。
- 通过调整阈值（0 到 1），绘制 TPR 对 FPR 的曲线。

#### 作用
- 评估模型区分正负类的能力，不依赖单一阈值。
- 对类别分布不敏感，适合比较不同模型的整体性能。

#### 示例
假设模型输出的正类概率和真实标签如下：
- 概率：[0.9, 0.7, 0.6, 0.4, 0.2]
- 真实标签：[1, 1, 0, 0, 1]
- 改变阈值，计算不同 FPR 和 TPR，绘制曲线。

---

### 3. AUC
#### 定义
AUC（曲线下面积）是 ROC 曲线下的面积，取值 0 到 1：
- AUC = 1：完美分类。
- AUC = 0.5：随机猜测。
- AUC < 0.5：分类器比随机还差。

#### 作用
- AUC 量化了 ROC 曲线的性能，是一个综合指标。
- 值越高，模型区分能力越强。

#### 示例
如果 ROC 曲线接近左上角，AUC 可能接近 1；如果接近对角线，AUC 约为 0.5。

---

### 独立代码示例
以下是一个通用的例子，使用 scikit-learn 生成 F1 分数、ROC 曲线和 AUC：

```python
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
import matplotlib.pyplot as plt

# 生成模拟二分类数据集
X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)

# 数据分割
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# 训练逻辑回归模型
model = LogisticRegression(random_state=42)
model.fit(X_train, y_train)

# 预测
y_pred = model.predict(X_test)  # 用于 F1 和混淆矩阵
y_prob = model.predict_proba(X_test)[:, 1]  # 用于 ROC 和 AUC

# 1. 混淆矩阵和 F1 分数
cm = confusion_matrix(y_test, y_pred)
print("混淆矩阵:\n", cm)
print("\n分类报告（包含 F1 分数）:\n", classification_report(y_test, y_pred))

# 2. 计算 ROC 曲线和 AUC
fpr, tpr, thresholds = roc_curve(y_test, y_prob)
roc_auc = auc(fpr, tpr)
print("AUC:", roc_auc)

# 3. 绘制 ROC 曲线
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC 曲线 (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='随机猜测')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('假阳性率 (FPR)')
plt.ylabel('真阳性率 (TPR)')
plt.title('ROC 曲线')
plt.legend(loc="lower right")
plt.grid(True)
plt.show()
```

---

### 输出解读
#### 混淆矩阵和 F1 分数
可能的输出：
```
混淆矩阵:
[[130  15]
 [ 20 135]]

分类报告:
              precision    recall  f1-score   support
       0       0.87      0.90      0.88       145
       1       0.90      0.87      0.89       155
accuracy                           0.88       300
```
- 类别 1 的 F1 分数为 0.89，表明模型在正类上的表现良好。

#### AUC
```
AUC: 0.93
```
- AUC = 0.93 表示模型整体区分能力很强。

#### ROC 曲线
- 图中蓝色曲线接近左上角，AUC 高于 0.5，优于随机猜测（灰色虚线）。

---

### 三者对比
| 指标       | 依赖阈值 | 适用场景                 | 优点                     | 缺点                   |
|------------|----------|--------------------------|--------------------------|------------------------|
| **F1 分数** | 是       | 不平衡数据，关注正类     | 平衡精确率和召回率       | 只反映单一阈值表现     |
| **ROC 曲线** | 否       | 比较模型整体性能         | 直观展示所有阈值表现     | 对不平衡数据解释需谨慎 |
| **AUC**     | 否       | 评估区分能力             | 单值总结，便于比较       | 不反映具体类别表现     |

---


## `confusion_matrix`（混淆矩阵）
（混淆矩阵）是分类任务中最常用的评估工具之一，特别适用于多分类和二分类问题，可以详细展示模型预测结果的 **正确性和错误类型**。

---

## 🧩 1. 什么是混淆矩阵？

对于一个二分类问题（如预测是/否、正/负），混淆矩阵长这样：

|               | 预测为正类（1） | 预测为负类（0） |
|---------------|------------------|------------------|
| 实际为正类（1） | TP（真正）         | FN（假负）         |
| 实际为负类（0） | FP（假正）         | TN（真负）         |

> 记忆方式：
- **T/F**：预测是否**正确**（True/False）
- **P/N**：预测的类别是**正类 or 负类**

---

## 🧪 2. 在 Python 中使用 `confusion_matrix`

```python
from sklearn.metrics import confusion_matrix

y_true = [1, 0, 1, 1, 0, 1, 0]
y_pred = [1, 0, 1, 0, 0, 1, 1]

cm = confusion_matrix(y_true, y_pred)
print(cm)
```

输出（可能是这样的）：

```
[[2 1]
 [1 3]]
```

解释为：

- TN = 2（实际为 0，预测也为 0）
- FP = 1（实际为 0，预测为 1）
- FN = 1（实际为 1，预测为 0）
- TP = 3（实际为 1，预测也为 1）

---

## 📊 3. 多分类问题的混淆矩阵

对于多类别（比如分类成 A、B、C），混淆矩阵就是一个 **n × n 的表格**：

- 行表示实际类别
- 列表示预测类别
- 对角线代表预测正确的数量

```python
from sklearn.metrics import confusion_matrix

y_true = [0, 1, 2, 1, 0, 2]
y_pred = [0, 2, 1, 1, 0, 2]

cm = confusion_matrix(y_true, y_pred)
print(cm)
```

输出：
```
[[2 0 0]
 [0 1 1]
 [0 1 1]]
```

---

## 🎯 4. 搭配可视化使用

```python
import seaborn as sns
import matplotlib.pyplot as plt

sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
```

---

## ✅ 常见指标从混淆矩阵中得到：

- 精度（Accuracy）: $ \frac{TP + TN}{总样本数} $
- 精确率（Precision）: $ \frac{TP}{TP + FP} $
- 召回率（Recall）: $ \frac{TP}{TP + FN} $
- F1 分数：调和平均 $ \frac{2 \cdot P \cdot R}{P + R} $

---





